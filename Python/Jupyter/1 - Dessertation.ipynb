{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1tEyCBZ0WW9SaQKWhveh5Et5-U7pU1E1d","authorship_tag":"ABX9TyNCslzpIHzYLF0MfmJ4vLcm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muVky-UauYTd","executionInfo":{"status":"ok","timestamp":1741002921158,"user_tz":0,"elapsed":10729,"user":{"displayName":"Muhammad Irfan","userId":"01070857291444674694"}},"outputId":"9fd02b6d-cedd-4db3-8db4-4034d94f4023"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["                                                text  \\\n","0  Remember when the Serious Antiracism Defeners ...   \n","1  this happened to Mister Bill so often @billkez...   \n","2  the amount of black people i saw on there get ...   \n","3  and then white ppl would just kinda shrug thei...   \n","4                        #Israel #US #enoughisenough   \n","\n","                                      processed_text  \n","0  remember serious antiracism defeners old site ...  \n","1              happened mister bill often bskysocial  \n","2  amount black people saw get called c wrong sid...  \n","3  white ppl would kinda shrug shoulder like that...  \n","4                      # israel # u # enoughisenough  \n"]}],"source":["!pip install emoji\n","!pip install nltk\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import emoji\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Dessertation/BlueSky - Riots.csv', encoding='latin-1')\n","df.head()\n","df_clean = df.copy()\n","df_clean = df_clean.dropna(subset=['text'])\n","\n","# Function to clean text\n","def clean_text(text):\n","    text = text.lower()  # Convert to lowercase\n","    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n","    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n","    text = re.sub(r\"[^\\w\\s#]\", \"\", text)  # Remove special characters (except hashtags)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n","    return text\n","\n","# Apply cleaning to the 'text' column\n","df_clean['cleaned_text'] = df_clean['text'].apply(clean_text)\n","\n","# Display the first few cleaned rows\n","df_clean[['text', 'cleaned_text']].head()\n","\n","df_clean.to_csv('/content/drive/MyDrive/Dessertation/BlueSky - Riots - Cleaned.csv', index=False)\n","\n","# Download NLTK resources\n","nltk.download('punkt_tab')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Initialize lemmatizer and stopword list\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to clean text\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n","    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n","    text = re.sub(r\"[^\\w\\s#]\", \"\", text)  # Remove special characters (except hashtags)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n","    return text\n","\n","# Function to handle negations\n","def handle_negation(text):\n","    words = text.split()\n","    new_words = []\n","    negation = False\n","\n","    for word in words:\n","        if word in [\"not\", \"no\", \"never\"]:\n","            negation = True\n","            new_words.append(word)\n","        elif negation:\n","            new_words.append(f\"not_{word}\")  # Combine negation with next word\n","            negation = False\n","        else:\n","            new_words.append(word)\n","\n","    return \" \".join(new_words)\n","\n","# Full preprocessing function\n","def preprocess_text(text):\n","    text = emoji.demojize(text)  # Convert emojis to text\n","    text = clean_text(text)  # Apply text cleaning\n","    text = handle_negation(text)  # Handle negations\n","\n","    tokens = word_tokenize(text)  # Tokenization\n","    tokens = [word for word in tokens if word not in stop_words]  # Stopword removal\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n","\n","    return \" \".join(tokens)\n","\n","# Apply preprocessing to dataset\n","df_clean['processed_text'] = df_clean['text'].apply(preprocess_text)\n","\n","# Save processed data\n","df_clean.to_csv(\"/content/drive/MyDrive/Dessertation/processed_bluesky_data.csv\", index=False)\n","\n","print(df_clean[['text', 'processed_text']].head())  # View some processed examples"]}]}